"""
Orbital VideoFactory - Audio-First Pipeline
============================================
Generates perfectly synced math tutorial videos.

Flow:
1. Load problem script (JSON)
2. Generate audio for each step via ElevenLabs
3. Measure audio durations
4. Save manifest with timings
5. Render Manim scene using exact audio timings
6. Combine intro + video + audio â†’ final output

Usage:
    python pipeline.py scripts/test_problem.json --voice allison --output my_video.mp4
"""

import os
import sys
import json
import subprocess
import argparse
from pathlib import Path
from pydub import AudioSegment

# Configuration
INTRO_DURATION = 2.0  # seconds - will be prepended to all videos
EXTRA_HOLD_PER_STEP = 0.8  # extra pause after each step to let viewer absorb
ANIMATION_RATIO = 0.35  # animation takes 35% of step duration (slower write)
FINAL_BOX_DURATION = 3.1  # time for final answer box: create (0.6) + hold (2.0) + fade (0.5)
# === TTS PROVIDER CONFIG ===
TTS_PROVIDER = os.environ.get("TTS_PROVIDER", "elevenlabs").lower()
ELEVEN_API_KEY = os.environ.get("ELEVEN_API_KEY", "sk_4abfbb388b66e23c7df0424e9228691ae139ab56a449e2a7")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

print(f"ðŸ”Š TTS Provider: {TTS_PROVIDER.upper()}")

# Voice IDs (ElevenLabs)
VOICES_ELEVENLABS = {
    "allison": "5jVVMAv2LzffTcLGarKh",
    "sarah": "EXAVITQu4vr4xnSDxMaL",
    "alice": "Xb7hH8MSUJpSbSDYk0k2",
    "daniel": "onwK4e9ZLuTAKqWW03F9",
    "george": "JBFqnCBsd6RMkjVDRZzb",
}

# Voice names (OpenAI)
VOICES_OPENAI = {
    "allison": "nova",    # Map allison to nova (warm female)
    "sarah": "nova",
    "alice": "shimmer",
    "daniel": "onyx",
    "george": "echo",
    "nova": "nova",
    "shimmer": "shimmer",
    "echo": "echo",
    "onyx": "onyx",
    "fable": "fable",
    "alloy": "alloy",
}


def generate_audio_steps(steps: list, voice: str, output_dir: str) -> list:
    """
    Generate audio for each step and return durations.
    Uses TTS_PROVIDER env var to choose between ElevenLabs and OpenAI.
    
    Returns list of dicts with: step_num, narration, latex, audio_path, duration
    """
    os.makedirs(output_dir, exist_ok=True)
    
    manifest = []
    
    for i, step in enumerate(steps):
        narration = step.get("narration", "")
        latex = step.get("latex", "")
        
        if not narration:
            continue
        
        audio_path = os.path.join(output_dir, f"step_{i:02d}.mp3")
        
        print(f"  Generating step {i}: {narration[:40]}...")
        
        if TTS_PROVIDER == "openai":
            # Use OpenAI TTS
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            voice_name = VOICES_OPENAI.get(voice.lower(), "nova")
            
            response = client.audio.speech.create(
                model="tts-1",
                voice=voice_name,
                input=narration,
                response_format="mp3"
            )
            response.stream_to_file(audio_path)
        else:
            # Use ElevenLabs
            from elevenlabs import ElevenLabs
            client = ElevenLabs(api_key=ELEVEN_API_KEY)
            voice_id = VOICES_ELEVENLABS.get(voice.lower(), voice)
            
            audio = client.text_to_speech.convert(
                text=narration,
                voice_id=voice_id,
                model_id="eleven_turbo_v2_5",
                output_format="mp3_44100_128"
            )
            
            with open(audio_path, "wb") as f:
                for chunk in audio:
                    f.write(chunk)
        
        # Measure duration
        audio_segment = AudioSegment.from_mp3(audio_path)
        duration = len(audio_segment) / 1000.0  # ms to seconds
        
        manifest.append({
            "step": i,
            "narration": narration,
            "latex": latex,
            "audio_path": audio_path,
            "duration": round(duration, 2)
        })
        
        print(f"    âœ“ {duration:.2f}s ({TTS_PROVIDER})")
    
    return manifest


def create_synced_manim_scene(manifest: list, output_path: str, intro_duration: float):
    """
    Create a Manim scene file with exact timings from audio manifest.
    """
    
    # Build the scene code dynamically
    scene_code = f'''"""
Auto-generated Manim scene with audio-synced timings.
Generated by Orbital VideoFactory Pipeline.
"""

from manim import *

ORBITAL_VIOLET = "#8B5CF6"
ORBITAL_CYAN = "#22D3EE"
NEON_GREEN = "#39FF14"   # Electric neon green for final answer box
NEON_CYAN = "#00FFFF"    # Bright cyan alternative

# Timing configuration
EXTRA_HOLD_PER_STEP = {EXTRA_HOLD_PER_STEP}  # extra pause after each step
ANIMATION_RATIO = {ANIMATION_RATIO}  # animation takes this % of step duration

class SyncedMathScene(Scene):
    """
    Math scene with timings synced to audio.
    Intro duration: {intro_duration}s (handled separately)
    """
    
    def construct(self):
        self.camera.background_color = "#000000"
        
        # Step timings from audio (in seconds)
        steps = {json.dumps(manifest, indent=8)}
        
        previous = None
        total_steps = len(steps)
        
        for i, step in enumerate(steps):
            latex = step["latex"]
            duration = step["duration"]
            
            # Calculate animation time (slower write at {ANIMATION_RATIO*100:.0f}% of duration)
            anim_time = max(1.2, duration * ANIMATION_RATIO)
            # Pause syncs with remaining audio time
            pause_time = max(0.3, duration - anim_time)
            
            # Create math object
            math = MathTex(latex, color=WHITE)
            math.scale(1.6)
            
            if previous is None:
                # First step: write on
                self.play(Write(math), run_time=anim_time)
            else:
                # Transform or fade transition
                self.play(
                    FadeOut(previous, shift=UP * 0.3),
                    run_time=0.4
                )
                self.play(Write(math), run_time=anim_time)
            
            # Pause to match audio duration
            self.wait(pause_time)
            
            # Extra hold time after each step (let viewer absorb)
            if i < total_steps - 1:  # not on final step
                self.wait(EXTRA_HOLD_PER_STEP)
            
            previous = math
        
        # Final answer: neon box highlight and hold
        if previous:
            NEON_GREEN = "#39FF14"
            box = SurroundingRectangle(
                previous, 
                color=NEON_GREEN,
                buff=0.3,
                stroke_width=4,
                corner_radius=0.1
            )
            self.play(Create(box), run_time=0.6)
            self.wait(2.0)  # hold final answer
            self.play(FadeOut(previous), FadeOut(box), run_time=0.5)


if __name__ == "__main__":
    print("Render with: manim -ql {{this_file}} SyncedMathScene")
'''
    
    with open(output_path, "w") as f:
        f.write(scene_code)
    
    print(f"  âœ“ Created Manim scene: {output_path}")


def render_manim_scene(scene_path: str, output_dir: str) -> str:
    """
    Render the Manim scene and return path to output video.
    """
    # Set up PATH for manim and latex
    env = os.environ.copy()
    env["PATH"] = f"{Path.home()}/Library/TinyTeX/bin/universal-darwin:{Path.home()}/Library/Python/3.9/bin:" + env.get("PATH", "")
    
    scene_path = os.path.abspath(scene_path)
    scene_dir = os.path.dirname(scene_path)
    scene_file = os.path.basename(scene_path)
    
    cmd = [
        "manim", "-ql",  # Low quality for speed (change to -qm or -qh for production)
        "--format", "mp4",
        scene_file,
        "SyncedMathScene"
    ]
    
    print(f"  Rendering Manim scene...")
    result = subprocess.run(cmd, cwd=scene_dir, env=env, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"  âŒ Manim error: {result.stderr}")
        return None
    
    # Find output file
    scene_name = Path(scene_path).stem
    video_path = Path(scene_dir) / "media" / "videos" / scene_name / "480p15" / "SyncedMathScene.mp4"
    
    if video_path.exists():
        print(f"  âœ“ Rendered: {video_path}")
        return str(video_path)
    
    return None


def combine_audio(manifest: list, intro_duration: float, output_path: str) -> str:
    """
    Combine all audio steps with intro silence, extra hold pauses, and final box time.
    """
    # Start with silence for intro
    combined = AudioSegment.silent(duration=int(intro_duration * 1000))
    
    total_steps = len(manifest)
    
    # Add each step's audio with extra hold silence after each (except last)
    for i, step in enumerate(manifest):
        audio = AudioSegment.from_mp3(step["audio_path"])
        combined += audio
        
        # Add extra hold silence after each step (except last)
        if i < total_steps - 1:
            combined += AudioSegment.silent(duration=int(EXTRA_HOLD_PER_STEP * 1000))
    
    # Add silence for final answer box animation (create + hold + fade)
    combined += AudioSegment.silent(duration=int(FINAL_BOX_DURATION * 1000))
    
    # Export
    combined.export(output_path, format="mp3")
    print(f"  âœ“ Combined audio: {output_path} ({len(combined)/1000:.1f}s)")
    
    return output_path


def create_intro_video(output_path: str, duration: float = 2.0):
    """
    Create a simple intro video with the Orbital logo.
    """
    # Use the factory root for logo
    factory_root = Path(__file__).parent
    logo_path = factory_root / "orbital-logo-with-name.png"
    
    intro_scene = f'''
from manim import *

class QuickIntro(Scene):
    def construct(self):
        self.camera.background_color = "#000000"
        
        # Load logo
        try:
            logo = ImageMobject("{logo_path}")
            logo.scale(0.5)
        except:
            # Fallback: text logo
            logo = Text("ORBITAL", font_size=72, color=WHITE)
        
        self.play(FadeIn(logo, scale=0.9), run_time=0.8)
        self.wait({duration - 1.3})
        self.play(FadeOut(logo), run_time=0.5)
'''
    
    scene_dir = os.path.dirname(os.path.abspath(output_path))
    scene_path = os.path.join(scene_dir, "quick_intro_scene.py")
    with open(scene_path, "w") as f:
        f.write(intro_scene)
    
    # Render
    env = os.environ.copy()
    env["PATH"] = f"{Path.home()}/Library/TinyTeX/bin/universal-darwin:{Path.home()}/Library/Python/3.9/bin:" + env.get("PATH", "")
    
    cmd = ["manim", "-ql", "quick_intro_scene.py", "QuickIntro"]
    subprocess.run(cmd, cwd=scene_dir, env=env, capture_output=True)
    
    # Find and copy output
    rendered = Path(scene_dir) / "media" / "videos" / "quick_intro_scene" / "480p15" / "QuickIntro.mp4"
    if rendered.exists():
        import shutil
        shutil.copy(rendered, output_path)
        print(f"  âœ“ Created intro: {output_path}")
        return output_path
    
    return None


def final_compose(intro_path: str, math_video_path: str, audio_path: str, output_path: str):
    """
    Combine intro video + math video + audio into final output.
    """
    # First concatenate videos
    concat_list = "/tmp/orbital_concat.txt"
    with open(concat_list, "w") as f:
        f.write(f"file '{intro_path}'\n")
        f.write(f"file '{math_video_path}'\n")
    
    temp_video = "/tmp/orbital_temp_video.mp4"
    
    cmd = [
        "ffmpeg", "-y",
        "-f", "concat", "-safe", "0",
        "-i", concat_list,
        "-c", "copy",
        temp_video
    ]
    subprocess.run(cmd, capture_output=True)
    
    # Merge video + audio
    cmd = [
        "ffmpeg", "-y",
        "-i", temp_video,
        "-i", audio_path,
        "-c:v", "copy",
        "-c:a", "aac",
        "-map", "0:v:0",
        "-map", "1:a:0",
        "-shortest",
        output_path
    ]
    subprocess.run(cmd, capture_output=True)
    
    print(f"\nâœ… Final video: {output_path}")
    return output_path


def run_pipeline(script_path: str, voice: str = "allison", output_name: str = None):
    """
    Run the full pipeline.
    """
    print("\n" + "="*60)
    print("ðŸš€ ORBITAL VIDEOFACTORY - Audio-First Pipeline")
    print("="*60 + "\n")
    
    # Load script
    with open(script_path) as f:
        script = json.load(f)
    
    meta = script.get("meta", {})
    steps = script.get("steps", [])
    
    print(f"ðŸ“„ Script: {script_path}")
    print(f"   Topic: {meta.get('topic', 'Unknown')}")
    print(f"   Steps: {len(steps)}")
    print(f"   Voice: {voice}")
    print()
    
    # Setup paths
    base_dir = Path(script_path).parent.parent
    job_name = Path(script_path).stem
    job_dir = base_dir / "jobs" / job_name
    os.makedirs(job_dir, exist_ok=True)
    
    # Step 1: Generate audio and get timings
    print("ðŸŽ™ï¸  Step 1: Generating audio...")
    audio_dir = job_dir / "audio"
    manifest = generate_audio_steps(steps, voice, str(audio_dir))
    
    # Save manifest
    manifest_path = job_dir / "manifest.json"
    with open(manifest_path, "w") as f:
        json.dump({"meta": meta, "intro_duration": INTRO_DURATION, "steps": manifest}, f, indent=2)
    print(f"  âœ“ Manifest saved: {manifest_path}")
    print()
    
    # Step 2: Create synced Manim scene
    print("ðŸŽ¬ Step 2: Creating synced Manim scene...")
    scene_path = job_dir / "synced_scene.py"
    create_synced_manim_scene(manifest, str(scene_path), INTRO_DURATION)
    print()
    
    # Step 3: Render Manim
    print("ðŸŽ¨ Step 3: Rendering animation...")
    math_video = render_manim_scene(str(scene_path), str(job_dir))
    if not math_video:
        print("âŒ Failed to render Manim scene")
        return None
    print()
    
    # Step 4: Create intro
    print("ðŸŽ¬ Step 4: Creating intro...")
    intro_path = job_dir / "intro.mp4"
    create_intro_video(str(intro_path), INTRO_DURATION)
    print()
    
    # Step 5: Combine audio with intro silence
    print("ðŸ”Š Step 5: Combining audio...")
    combined_audio = job_dir / "combined_audio.mp3"
    combine_audio(manifest, INTRO_DURATION, str(combined_audio))
    print()
    
    # Step 6: Final composition
    print("ðŸŽ¬ Step 6: Final composition...")
    output_name = output_name or f"{job_name}_final.mp4"
    output_path = base_dir / "output" / output_name
    os.makedirs(output_path.parent, exist_ok=True)
    
    final_compose(str(intro_path), math_video, str(combined_audio), str(output_path))
    
    print("\n" + "="*60)
    print(f"âœ… COMPLETE! Video saved to:")
    print(f"   {output_path}")
    print("="*60 + "\n")
    
    return str(output_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Orbital VideoFactory Pipeline")
    parser.add_argument("script", help="Path to problem script JSON")
    parser.add_argument("--voice", "-v", default="allison", help="Voice name or ID")
    parser.add_argument("--output", "-o", help="Output filename")
    
    args = parser.parse_args()
    
    result = run_pipeline(args.script, args.voice, args.output)
    
    if result:
        # Open the result
        subprocess.run(["open", result])
